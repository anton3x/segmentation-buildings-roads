{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.amp import GradScaler\n",
    "from tqdm import tqdm\n",
    "from torch import autocast\n",
    "from torchmetrics import JaccardIndex, F1Score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Image_dir_train = \"Datasets/Combined_Augmented/train/images\"\n",
    "Mask_dir_train = \"Datasets/Combined_Augmented/train/labels\"\n",
    "Image_dir_val = \"Datasets/Combined_Augmented/val/images\"\n",
    "Mask_dir_val = \"Datasets/Combined_Augmented/val/labels\"\n",
    "Image_dir_test = \"Datasets/CITY_OSM/test/images\"\n",
    "Mask_dir_test= \"Datasets/CITY_OSM/test/labels\"\n",
    "batch_size = 6\n",
    "patience = 20\n",
    "epochs = 65\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ],
   "id": "24ab6563ebc94b47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocess_input = smp.encoders.get_preprocessing_fn('tu-maxvit_large_tf_512', pretrained='imagenet')\n",
    "\n",
    "# Classe do Dataset utilizando Albumentations\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, img_paths: list, mask_paths: list):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Leitura e pré-processamento básico\n",
    "        image = cv2.imread(self.img_paths[index])\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image,(512,512))\n",
    "        image = image.astype('float32') / 255.0\n",
    "\n",
    "        image = preprocess_input(image)\n",
    "\n",
    "        mask = cv2.imread(self.mask_paths[index],cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask,(512,512))\n",
    "        mask = mask.astype('int8')\n",
    "        mask = np.expand_dims(mask,axis=0) # (1,512,512)\n",
    "\n",
    "        image = torch.tensor(image).permute(2,0,1)\n",
    "        mask = torch.tensor(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "# Listando os caminhos dos arquivos\n",
    "train_images = sorted(glob(os.path.join(Image_dir_train, \"*png\")))\n",
    "train_mask   = sorted(glob(os.path.join(Mask_dir_train, \"*png\")))\n",
    "\n",
    "val_images = sorted(glob(os.path.join(Image_dir_val, \"*png\")))\n",
    "val_mask   = sorted(glob(os.path.join(Mask_dir_val, \"*png\")))\n",
    "\n",
    "test_images = sorted(glob(os.path.join(Image_dir_test, \"*png\")))\n",
    "test_mask   = sorted(glob(os.path.join(Mask_dir_test, \"*png\")))\n",
    "\n",
    "# Criando os datasets com as transformações apropriadas\n",
    "train_dataset = SegmentationDataset(train_images, train_mask)\n",
    "val_dataset   = SegmentationDataset(val_images, val_mask)\n",
    "test_dataset  = SegmentationDataset(test_images, test_mask)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "3d6332fc017fa888",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = smp.Unet(encoder_name='tu-maxvit_large_tf_512',encoder_weights='imagenet',in_channels=3,classes=3)\n",
    "model.to(device)\n",
    "criterion_ce = torch.nn.CrossEntropyLoss()  # Cross-Entropy Loss\n",
    "criterion_dice = smp.losses.DiceLoss(mode=\"multiclass\")  # Dice Loss\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.0001)\n",
    "\n",
    "# Agendador de aprendizado com decaimento polinomial\n",
    "decay_steps = epochs * len(train_loader)  # Número total de iterações de treinamento\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(optimizer, total_iters=decay_steps, power=2.0)"
   ],
   "id": "c1f407aa599f8782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Criando listas vazias para armazenar os valores\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_iou': [],\n",
    "    'val_iou': [],\n",
    "    'train_f1': [],\n",
    "    'val_f1': []\n",
    "}"
   ],
   "id": "52bd338a3c178008",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_loss = float(\"inf\")\n",
    "counter = 0\n",
    "train_loss = 0.0\n",
    "train_iou = 0.0\n",
    "train_f1 = 0.0\n",
    "val_loss = 0.0\n",
    "val_iou = 0.0\n",
    "val_f1 = 0.0\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (images, mask) in enumerate(tqdm(train_loader, desc=f\"Época {epoch+1}/{epochs}\"), 1):\n",
    "        images,mask = images.to(device).float(), mask.to(device)\n",
    "        mask = mask.to(device).squeeze(1).long()\n",
    "        mask = torch.clamp(mask, 0, 2) #garantir que os pixeis estao entre [0, 2]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'): #optimizar o treino mexendo na precision\n",
    "            output = model(images)\n",
    "            loss_ce = criterion_ce(output, mask) #os dois tipos de erro\n",
    "            loss_dice = criterion_dice(output, mask)\n",
    "            loss = 0.5 * loss_ce + 0.5 * loss_dice\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        scheduler.step()  # Atualizar taxa de aprendizado no final de cada batch\n",
    "\n",
    "        # Para dados multiclass, aplica-se a argmax para obter probabilidades\n",
    "        output_probs = torch.argmax(output, dim=1)\n",
    "        # Calcula as estatísticas para dados multiclasses\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(output_probs, mask, mode='multiclass', num_classes=3)\n",
    "        # Calcula IoU e F1 Score (Dice) com redução \"micro\"\n",
    "        batch_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        batch_f1 = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        train_iou += batch_iou.item()\n",
    "        train_f1 += batch_f1.item()\n",
    "\n",
    "        #tqdm.write(f\"Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, mask in val_loader:\n",
    "            images,mask = images.to(device).float(), mask.to(device)\n",
    "            mask = mask.to(device).squeeze(1).long()\n",
    "            mask = torch.clamp(mask, 0, 2) #garantir que os pixeis estao entre [0, 2]\n",
    "            output = model(images)\n",
    "\n",
    "            loss_ce = criterion_ce(output, mask)\n",
    "            loss_dice = criterion_dice(output, mask)\n",
    "            loss = 0.5 * loss_ce + 0.5 * loss_dice\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            output_probs = torch.argmax(output, dim=1)\n",
    "            tp, fp, fn, tn = smp.metrics.get_stats(output_probs, mask, mode='multiclass', num_classes=3)\n",
    "            batch_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "            batch_f1 = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "            val_iou += batch_iou.item()\n",
    "            val_f1 += batch_f1.item()\n",
    "\n",
    "\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    train_iou /= len(train_loader)\n",
    "    train_f1 /= len(train_loader)\n",
    "    val_iou /= len(val_loader)\n",
    "    val_f1 /= len(val_loader)\n",
    "\n",
    "    # Armazena os valores nas listas para plotagem posterior\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_iou'].append(train_iou)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_f1'].append(val_f1)\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{epochs}, Train Loss: {train_loss:.4f}, Train IoU: {train_iou:.4f}, Train F1: {train_f1:.4f}, Scheduler: {scheduler.get_last_lr()}\")\n",
    "    print(f\"             Val Loss: {val_loss:.4f}, Val IoU: {val_iou:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_loss < best_loss: #guardar os modelos se tiverem um val_loss menor\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"UNetMaxVitLCombinedAug.pth\")\n",
    "        print(f\"saveing best model with val_loss {val_loss} at UNetMaxVitLCombinedAug.pth\")\n",
    "    else: #early stopping ao final de x epocas sem melhorar\n",
    "        counter += 1\n",
    "        print(f\"EarlyStopping: {counter}/{patience}\")\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping ativado após {epoch+1} épocas\")\n",
    "            break"
   ],
   "id": "393c33f580d75ed7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Plot do Loss\n",
    "epochs = range(1, len(history['train_loss']) + 1 )\n",
    "loss = history['train_loss']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "5f90a51226027b5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot do IoU\n",
    "epochs = range(1, len(history[\"val_iou\"]) + 1)\n",
    "iou = history[\"train_iou\"]\n",
    "val_iou = history[\"val_iou\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, iou, \"bo\", label=\"Training IoU\")\n",
    "plt.plot(epochs, val_iou, \"b\", label=\"Validation IoU\")\n",
    "plt.title(\"Training and validation IoU\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "26bdca01fc8c6afa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot do F1-Score\n",
    "epochs = range(1, len(history[\"val_f1\"]) + 1)\n",
    "f1 = history[\"train_f1\"]\n",
    "val_f1 = history[\"val_f1\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, f1, \"bo\", label=\"Training F1-Score\")\n",
    "plt.plot(epochs, val_f1, \"b\", label=\"Validation F1-Score\")\n",
    "plt.title(\"Training and validation F1-Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "8cbdb5d026dfda9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def infer_and_visualize(model,image_path,device):\n",
    "    image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image,(512, 512))\n",
    "    image_tensor = image.astype('float32') / 255.0 #(512,512,3)\n",
    "    image_tensor = preprocess_input(image_tensor)\n",
    "\n",
    "    image_tensor = torch.tensor(image_tensor).permute(2,0,1).unsqueeze(0).to(device).float() # (1,3,512,512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        output = torch.softmax(output, dim=1)\n",
    "        output = output.argmax(dim=1).cpu().squeeze().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Mask\")\n",
    "    plt.imshow(output)\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"overlap\")\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(output,cmap='jet',alpha=0.5)\n",
    "\n",
    "\n",
    "    plt.show()"
   ],
   "id": "4f1b50b86682f04a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.load_state_dict(torch.load(\"UNetMaxVitLCombinedAug.pth\"))",
   "id": "219a7abed3b09c3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.eval()",
   "id": "32e895e11bb81155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "infer_and_visualize(model,\"Datasets/PrivateDataset/test/images/256.png\",device)",
   "id": "92332fc4b679eb32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    # Cria os objetos de métricas para 3 classes com average='none' para obter o valor de cada classe\n",
    "    jaccard = JaccardIndex(num_classes=3, average='none', task=\"multiclass\").to(device)\n",
    "    f1score = F1Score(num_classes=3, average='none', task=\"multiclass\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, mask in tqdm(test_loader, desc=\"Teste\"):\n",
    "            images, mask = images.to(device).float(), mask.to(device)\n",
    "            mask = mask.squeeze(1).long()  # Remove a dimensão extra, se necessário\n",
    "\n",
    "            output = model(images)\n",
    "            loss_ce = criterion_ce(output, mask)\n",
    "            loss_dice = criterion_dice(output, mask)\n",
    "            loss = 0.5 * loss_ce + 0.5 * loss_dice\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            output_probs = torch.argmax(output, dim=1)\n",
    "\n",
    "            # Atualiza as métricas para o batch atual\n",
    "            jaccard.update(output_probs, mask)\n",
    "            f1score.update(output_probs, mask)\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    per_class_iou = jaccard.compute()   # Tensor com o IoU de cada classe\n",
    "    per_class_f1 = f1score.compute()      # Tensor com o F1 score de cada classe\n",
    "\n",
    "    mean_iou = per_class_iou.mean()\n",
    "    mean_f1 = per_class_f1.mean()\n",
    "\n",
    "    # Exibe os resultados\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    for i in range(3):\n",
    "        print(f\"Classe {i}: IoU = {per_class_iou[i]:.4f}, F1 = {per_class_f1[i]:.4f}\")\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"Mean F1: {mean_f1:.4f}\")\n",
    "\n",
    "    return avg_loss, per_class_iou, mean_iou, per_class_f1, mean_f1\n",
    "\n",
    "# Exemplo de uso:\n",
    "test_loss, per_class_iou, mean_iou, per_class_f1, mean_f1 = test_model(model, test_loader, device)"
   ],
   "id": "37d6a2069a6713da",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
